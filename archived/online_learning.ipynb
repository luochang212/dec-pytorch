{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dce7b33e-299a-4775-b66f-991104aca57f",
   "metadata": {},
   "source": [
    "# 在线学习\n",
    "\n",
    "我希望当新一批 embeddings 进入时，只进行少量的训练。既让模型适应新数据，又尽量不使原本的 embedding - label 映射发生偏移。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ceb0b88d-ab27-4f7a-a76b-374631f8f0ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T20:57:38.333415Z",
     "iopub.status.busy": "2025-03-07T20:57:38.332414Z",
     "iopub.status.idle": "2025-03-07T20:57:38.338660Z",
     "shell.execute_reply": "2025-03-07T20:57:38.337647Z",
     "shell.execute_reply.started": "2025-03-07T20:57:38.333415Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "18e8dcbe-63ba-4819-9946-eb4e81ef7077",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T20:59:00.689033Z",
     "iopub.status.busy": "2025-03-07T20:59:00.688057Z",
     "iopub.status.idle": "2025-03-07T20:59:00.714059Z",
     "shell.execute_reply": "2025-03-07T20:59:00.713006Z",
     "shell.execute_reply.started": "2025-03-07T20:59:00.689033Z"
    }
   },
   "outputs": [],
   "source": [
    "# 设备配置\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 超参数配置\n",
    "config = {\n",
    "    \"dims\": [768, 512, 256, 64, 10],\n",
    "    \"n_clusters\": 100,\n",
    "    \"pretrain_epochs\": 100,\n",
    "    \"maxiter\": 2000,\n",
    "    \"batch_size\": 256,\n",
    "    \"update_interval\": 100,\n",
    "    \"tol\": 0.001,\n",
    "    \"alpha\": 1.0,\n",
    "    \"save_dir\": \"./model\"\n",
    "}\n",
    "\n",
    "# 加载模型\n",
    "def load_dec_model(config, device):\n",
    "    model = DEC().to(device)\n",
    "    model.load_state_dict(\n",
    "        torch.load(\n",
    "            f\"{config['save_dir']}/best_model.pth\", \n",
    "            map_location=device,\n",
    "            weights_only=True))\n",
    "\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "# 自编码器\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, dims):\n",
    "        super().__init__()\n",
    "        # 编码器\n",
    "        encoder_layers = []\n",
    "        for i in range(len(dims)-1):\n",
    "            encoder_layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if i != len(dims)-2:\n",
    "                encoder_layers.append(nn.ReLU())\n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # 解码器（对称结构）\n",
    "        decoder_layers = []\n",
    "        for i in reversed(range(len(dims)-1)):\n",
    "            decoder_layers.append(nn.Linear(dims[i+1], dims[i]))\n",
    "            if i != 0:\n",
    "                decoder_layers.append(nn.ReLU())\n",
    "        self.decoder = nn.Sequential(*decoder_layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h = self.encoder(x)\n",
    "        h = F.normalize(h, p=2, dim=1)  # 添加L2归一化\n",
    "        return self.decoder(h)\n",
    "\n",
    "# 数值稳定的聚类层\n",
    "class ClusteringLayer(nn.Module):\n",
    "    def __init__(self, n_clusters, alpha=1.0):\n",
    "        super().__init__()\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.clusters = nn.Parameter(torch.Tensor(n_clusters, config[\"dims\"][-1]))\n",
    "        nn.init.xavier_normal_(self.clusters)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # 稳定计算距离\n",
    "        x = x.unsqueeze(1)  # [bs, 1, feat_dim]\n",
    "        clusters = self.clusters.unsqueeze(0)  # [1, n_clusters, feat_dim]\n",
    "        dist = torch.sum((x - clusters)**2, dim=2) / self.alpha  # [bs, n_clusters]\n",
    "        \n",
    "        # 数值稳定的soft分配\n",
    "        q = 1.0 / (1.0 + dist)\n",
    "        q = q ** ((self.alpha + 1.0) / 2.0)\n",
    "        return q / torch.sum(q, dim=1, keepdim=True)\n",
    "\n",
    "class DEC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.autoencoder = Autoencoder(config[\"dims\"]).to(device)\n",
    "        self.encoder = self.autoencoder.encoder\n",
    "        self.clustering = ClusteringLayer(config[\"n_clusters\"], config[\"alpha\"])\n",
    "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def target_distribution(self, q):\n",
    "        \"\"\"修正后的目标分布计算\"\"\"\n",
    "        p = q**2 / torch.sum(q, dim=0)\n",
    "        return (p.t() / torch.sum(p.t(), dim=1, keepdim=True)).t().detach()  # 关键修正\n",
    "    \n",
    "    def pretrain(self, data_loader):\n",
    "        optimizer = optim.Adam(self.parameters())\n",
    "        criterion = nn.MSELoss()\n",
    "        self.train()\n",
    "        \n",
    "        for epoch in range(config[\"pretrain_epochs\"]):\n",
    "            total_loss = 0.0\n",
    "            for idx, x in data_loader:\n",
    "                x = x.to(device)\n",
    "                optimizer.zero_grad()\n",
    "                x_recon = self.autoencoder(x)\n",
    "                loss = criterion(x_recon, x)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            \n",
    "            print(f\"Pretrain Epoch {epoch+1}/{config['pretrain_epochs']}, Loss: {total_loss/len(data_loader):.4f}\")\n",
    "    \n",
    "    def fit(self, X, y_true=None):\n",
    "        # 数据准备（带索引）\n",
    "        dataset = TensorDataset(torch.arange(len(X)), torch.from_numpy(X.astype(np.float32)))\n",
    "        pretrain_loader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=True)\n",
    "        \n",
    "        # 预训练阶段\n",
    "        self.pretrain(pretrain_loader)\n",
    "        \n",
    "        # 初始化聚类中心\n",
    "        with torch.no_grad():\n",
    "            full_loader = DataLoader(dataset, batch_size=1024, shuffle=False)\n",
    "            features, indices = [], []\n",
    "            for idx, x in full_loader:\n",
    "                features.append(self.encoder(x.to(device)).cpu())\n",
    "                indices.append(idx)\n",
    "            features = torch.cat(features).numpy()\n",
    "            indices = torch.cat(indices).numpy()\n",
    "            \n",
    "            kmeans = KMeans(n_clusters=config[\"n_clusters\"], n_init=20)\n",
    "            y_pred = kmeans.fit_predict(features)\n",
    "            self.clustering.clusters.data = torch.tensor(kmeans.cluster_centers_, device=device)\n",
    "\n",
    "        # 准备聚类优化\n",
    "        optimizer = optim.SGD(self.parameters(), lr=0.01, momentum=0.9)\n",
    "        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=500, gamma=0.5)\n",
    "        y_pred_last = y_pred.copy()\n",
    "        best_acc = 0.0\n",
    "\n",
    "        # 主训练循环\n",
    "        cluster_loader = DataLoader(dataset, batch_size=config[\"batch_size\"], shuffle=False)\n",
    "        with tqdm(total=config[\"maxiter\"], desc=\"Clustering\") as pbar:\n",
    "            for ite in range(config[\"maxiter\"]):\n",
    "                # 更新目标分布\n",
    "                if ite % config[\"update_interval\"] == 0:\n",
    "                    with torch.no_grad():\n",
    "                        # q = self.clustering(self.encoder(torch.from_numpy(X).float().to(device)))\n",
    "                        # p = self.target_distribution(q)\n",
    "\n",
    "                        q_list = []\n",
    "                        for idx, x in DataLoader(dataset, batch_size=1024, shuffle=False):\n",
    "                            x = x.to(device)\n",
    "                            q_batch = self.clustering(self.encoder(x))\n",
    "                            q_list.append(q_batch)\n",
    "                        q = torch.cat(q_list, dim=0)  # 分批次计算全量q\n",
    "                        p = self.target_distribution(q)  # 使用修正后的目标分布\n",
    "\n",
    "                        # 计算聚类指标\n",
    "                        y_pred = q.argmax(1).cpu().numpy()\n",
    "                        if y_true is not None:\n",
    "                            current_acc = acc(y_true, y_pred)\n",
    "                            current_nmi = nmi(y_true, y_pred)\n",
    "                            current_ari = ari(y_true, y_pred)\n",
    "                            pbar.set_postfix(ACC=current_acc, NMI=current_nmi, ARI=current_ari)\n",
    "                            \n",
    "                            if current_acc > best_acc:\n",
    "                                best_acc = current_acc\n",
    "                                torch.save(self.state_dict(), f\"{config['save_dir']}/best_model.pth\")\n",
    "                        \n",
    "                        # 检查收敛\n",
    "                        delta_label = np.sum(y_pred != y_pred_last) / X.shape[0]\n",
    "                        if delta_label < config[\"tol\"]:\n",
    "                            print(f\"\\nConverged at iteration {ite}\")\n",
    "                            break\n",
    "                        y_pred_last = y_pred.copy()\n",
    "                \n",
    "                # 批量训练\n",
    "                for idx, x in cluster_loader:\n",
    "                    x = x.to(device)\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # 前向计算\n",
    "                    z = self.encoder(x)\n",
    "                    q_batch = self.clustering(z)\n",
    "                    log_q = self.log_softmax(q_batch)\n",
    "                    \n",
    "                    # 获取对应p值\n",
    "                    p_batch = p[idx].to(device)\n",
    "                    \n",
    "                    # 计算损失\n",
    "                    loss = F.kl_div(log_q, p_batch, reduction='batchmean')\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                scheduler.step()\n",
    "                pbar.update(1)\n",
    "\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c23c533-583a-46d9-9d19-c7fd1031b8a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T21:00:26.548824Z",
     "iopub.status.busy": "2025-03-07T21:00:26.547825Z",
     "iopub.status.idle": "2025-03-07T21:00:26.560131Z",
     "shell.execute_reply": "2025-03-07T21:00:26.558822Z",
     "shell.execute_reply.started": "2025-03-07T21:00:26.548824Z"
    }
   },
   "outputs": [],
   "source": [
    "class DynamicClusteringLayer(ClusteringLayer):\n",
    "    def __init__(self, n_clusters, alpha=1.0, expansion_threshold=0.3):\n",
    "        super().__init__(n_clusters, alpha)\n",
    "        self.expansion_threshold = expansion_threshold\n",
    "        self.history_centers_mask = None  # 标记历史中心\n",
    "\n",
    "    def dynamic_update(self, new_embeddings):\n",
    "        \"\"\"动态扩展聚类中心\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # 计算新数据到最近中心的距离\n",
    "            dists = torch.cdist(new_embeddings, self.clusters)\n",
    "            min_dists, _ = torch.min(dists, dim=1)\n",
    "            \n",
    "            # 识别需要新建中心的数据\n",
    "            new_centers = new_embeddings[min_dists > self.expansion_threshold]\n",
    "            if len(new_centers) > 0:\n",
    "                # 扩展聚类中心\n",
    "                updated_centers = torch.cat([self.clusters, new_centers], dim=0)\n",
    "                self.clusters = nn.Parameter(updated_centers)\n",
    "                self.n_clusters += len(new_centers)\n",
    "                \n",
    "                # 更新历史中心标记\n",
    "                new_mask = torch.ones(self.n_clusters, dtype=bool)\n",
    "                if self.history_centers_mask is not None:\n",
    "                    new_mask[:len(self.history_centers_mask)] = self.history_centers_mask\n",
    "                self.history_centers_mask = new_mask\n",
    "\n",
    "def elastic_loss(current_centers, original_centers, fisher_matrix, lambda_=0.5):\n",
    "    \"\"\"基于EWC的弹性约束\"\"\"\n",
    "    delta = current_centers - original_centers\n",
    "    return lambda_ * torch.sum(fisher_matrix * delta**2)\n",
    "\n",
    "def compute_fisher(model, data_loader, samples=1000):\n",
    "    fisher = torch.zeros_like(model.clustering.clusters)\n",
    "    \n",
    "    model.eval()\n",
    "    for idx, (x, _) in enumerate(data_loader):\n",
    "        if idx * data_loader.batch_size > samples:\n",
    "            break\n",
    "        x = x.to(device)\n",
    "        q = model.clustering(model.encoder(x))\n",
    "        prob = q.max(dim=1)[0]\n",
    "        prob.backward(torch.ones_like(prob))\n",
    "        fisher += model.clustering.clusters.grad.pow(2)\n",
    "        model.zero_grad()\n",
    "    return fisher / len(data_loader)\n",
    "\n",
    "def online_update(model, new_embeddings, original_centers, fisher_matrix,\n",
    "                  lr=0.01, max_iters=50, batch_size=256):\n",
    "    # 冻结编码器参数\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    # 动态扩展聚类中心\n",
    "    model.clustering.dynamic_update(new_embeddings)\n",
    "    \n",
    "    # 配置优化器（仅优化聚类中心）\n",
    "    optimizer = optim.SGD(\n",
    "        [{'params': model.clustering.clusters, 'lr': lr}], \n",
    "        momentum=0.9\n",
    "    )\n",
    "    \n",
    "    # 准备数据\n",
    "    dataset = TensorDataset(torch.from_numpy(new_embeddings))\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    # 训练循环\n",
    "    for epoch in range(max_iters):\n",
    "        for batch in loader:\n",
    "            x = batch[0].to(device)\n",
    "            \n",
    "            # 前向计算\n",
    "            z = model.encoder(x)\n",
    "            q = model.clustering(z)\n",
    "            \n",
    "            # 目标分布计算（仅用新数据）\n",
    "            p = model.target_distribution(q)\n",
    "            \n",
    "            # 损失计算\n",
    "            kl_loss = F.kl_div(torch.log(q), p, reduction='batchmean')\n",
    "            ewc_loss = elastic_loss(\n",
    "                model.clustering.clusters[:len(original_centers)],\n",
    "                original_centers,\n",
    "                fisher_matrix\n",
    "            )\n",
    "            \n",
    "            total_loss = kl_loss + ewc_loss\n",
    "            \n",
    "            # 反向传播\n",
    "            optimizer.zero_grad()\n",
    "            total_loss.backward()\n",
    "            \n",
    "            # 限制历史中心梯度\n",
    "            if model.clustering.history_centers_mask is not None:\n",
    "                mask = model.clustering.history_centers_mask.to(device)\n",
    "                model.clustering.clusters.grad[mask] *= 0.3  # 衰减历史中心更新\n",
    "            \n",
    "            optimizer.step()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "faaad33d-c7d5-4147-9b1a-97d62633020f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T21:00:27.362255Z",
     "iopub.status.busy": "2025-03-07T21:00:27.361257Z",
     "iopub.status.idle": "2025-03-07T21:00:27.388837Z",
     "shell.execute_reply": "2025-03-07T21:00:27.388837Z",
     "shell.execute_reply.started": "2025-03-07T21:00:27.362255Z"
    }
   },
   "outputs": [],
   "source": [
    "# 加载基础模型\n",
    "base_model = load_dec_model(config, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d269f7ce-3508-4511-a34e-aebdbe9d0fbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-07T21:00:27.760668Z",
     "iopub.status.busy": "2025-03-07T21:00:27.760668Z",
     "iopub.status.idle": "2025-03-07T21:00:27.780172Z",
     "shell.execute_reply": "2025-03-07T21:00:27.779666Z",
     "shell.execute_reply.started": "2025-03-07T21:00:27.760668Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'legacy_data_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 记录初始聚类中心和历史Fisher信息\u001b[39;00m\n\u001b[0;32m      2\u001b[0m original_centers \u001b[38;5;241m=\u001b[39m base_model\u001b[38;5;241m.\u001b[39mclustering\u001b[38;5;241m.\u001b[39mclusters\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[1;32m----> 3\u001b[0m fisher_matrix \u001b[38;5;241m=\u001b[39m compute_fisher(base_model, \u001b[43mlegacy_data_loader\u001b[49m)  \u001b[38;5;66;03m# 预计算Fisher信息\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'legacy_data_loader' is not defined"
     ]
    }
   ],
   "source": [
    "# 记录初始聚类中心和历史Fisher信息\n",
    "original_centers = base_model.clustering.clusters.clone().detach()\n",
    "fisher_matrix = compute_fisher(base_model, legacy_data_loader)  # 预计算Fisher信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53227703-175f-4445-b395-70dd98452eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_model = online_update(\n",
    "    base_model, \n",
    "    new_embeddings=new_emb,\n",
    "    original_centers=original_centers,\n",
    "    fisher_matrix=fisher_matrix\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc69918c-46fd-4d18-aaf3-aa6bd2165ae2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
